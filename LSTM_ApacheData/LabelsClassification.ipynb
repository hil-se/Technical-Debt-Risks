{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Loading </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corr=pd.read_csv(\"apache-ant-corr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>projectname</th>\n",
       "      <th>classification</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// FIXME formatters are not thread-safe</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// XXX: (Jon Skeet) The comment \"if it hasn't ...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// I hate to admit it, but we don't know what ...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// Just a note: StarTeam has a status for NEW ...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// the generated classes must not be added in ...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       projectname classification  \\\n",
       "0   0  apache-ant-1.7.0         DEFECT   \n",
       "1   1  apache-ant-1.7.0         DEFECT   \n",
       "2   2  apache-ant-1.7.0         DEFECT   \n",
       "3   3  apache-ant-1.7.0         DEFECT   \n",
       "4   4  apache-ant-1.7.0         DEFECT   \n",
       "\n",
       "                                            Abstract label  \n",
       "0            // FIXME formatters are not thread-safe   yes  \n",
       "1  // XXX: (Jon Skeet) The comment \"if it hasn't ...   yes  \n",
       "2  // I hate to admit it, but we don't know what ...   yes  \n",
       "3  // Just a note: StarTeam has a status for NEW ...   yes  \n",
       "4  // the generated classes must not be added in ...   yes  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_corr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4098, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_corr.loc[data_corr.label=='yes', 'label'].count()\n",
    "#(data_corr.label=='yes').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3963"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_corr.query('label == \"no\"').label.count()\n",
    "#data_corr.label.value_counts().no  \n",
    "#Note value_counts() is used only with series type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments=data_corr[['Abstract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>// FIXME formatters are not thread-safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>// XXX: (Jon Skeet) The comment \"if it hasn't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>// I hate to admit it, but we don't know what ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>// Just a note: StarTeam has a status for NEW ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>// the generated classes must not be added in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract\n",
       "0            // FIXME formatters are not thread-safe\n",
       "1  // XXX: (Jon Skeet) The comment \"if it hasn't ...\n",
       "2  // I hate to admit it, but we don't know what ...\n",
       "3  // Just a note: StarTeam has a status for NEW ...\n",
       "4  // the generated classes must not be added in ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=data_corr[['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label\n",
       "0   yes\n",
       "1   yes\n",
       "2   yes\n",
       "3   yes\n",
       "4   yes"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list=[]\n",
    "for i, row in labels.iterrows():\n",
    "    labels_list.append(row['label'])\n",
    "    \n",
    "comments_list=[]\n",
    "for i, row in comments.iterrows():\n",
    "    comments_list.append(row['Abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Start modelling </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(num_words, sequence_length, test_size=0.25, oov_token=None):\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(comments_list)\n",
    "    X = tokenizer.texts_to_sequences(comments_list)\n",
    "    X,l= np.array(X), np.array(labels)\n",
    "    \n",
    "    X = pad_sequences(X, maxlen=sequence_length)\n",
    "    label_encoder = LabelEncoder()\n",
    "    vec = label_encoder.fit_transform(l)\n",
    "    y=to_categorical(vec)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=1)\n",
    "    data = {}\n",
    "    data[\"X_train\"] = X_train\n",
    "    data[\"X_test\"]= X_test\n",
    "    data[\"y_train\"] = y_train\n",
    "    data[\"y_test\"] = y_test\n",
    "    data[\"tokenizer\"] = tokenizer\n",
    "    data[\"int2label\"] =  {0: \"no\", 1: \"yes\"}\n",
    "    data[\"label2int\"] = {\"no\": 0, \"yes\": 1}\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Build Embeddings </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_vectors(word_index, embedding_size=100):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_size))\n",
    "    with open(f\"data/glove.6B.{embedding_size}d.txt\", encoding=\"utf8\") as f:\n",
    "        for line in tqdm(f, \"Reading GloVe\"):\n",
    "            values = line.split()\n",
    "            # get the word as the first word in the line\n",
    "            word = values[0]\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                # get the vectors as the remaining values in the line\n",
    "                embedding_matrix[idx] = np.array(values[1:], dtype=\"float32\")\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(word_index, units=128, n_layers=1, cell=LSTM, bidirectional=False,\n",
    "                embedding_size=100, sequence_length=100, dropout=0.3, \n",
    "                loss=\"categorical_crossentropy\", optimizer=\"adam\", \n",
    "                output_length=2):\n",
    "    \"\"\"Constructs a RNN model given its parameters\"\"\"\n",
    "    embedding_matrix = get_embedding_vectors(word_index, embedding_size)\n",
    "    model = Sequential()\n",
    "    # add the embedding layer\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "              embedding_size,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              input_length=sequence_length))\n",
    "    for i in range(n_layers):\n",
    "        if i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # first layer or hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(output_length, activation=\"softmax\"))\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of words in each sentence\n",
    "SEQUENCE_LENGTH = 300\n",
    "# N-Dimensional GloVe embedding vectors\n",
    "EMBEDDING_SIZE = 300\n",
    "# number of words to use, discarding the rest\n",
    "N_WORDS = 10000\n",
    "# out of vocabulary token\n",
    "OOV_TOKEN = None\n",
    "# 30% testing set, 70% training set\n",
    "TEST_SIZE = 0.3\n",
    "# number of CELL layers\n",
    "N_LAYERS = 1\n",
    "# the RNN cell to use, LSTM in this case\n",
    "RNN_CELL = LSTM\n",
    "# whether it's a bidirectional RNN\n",
    "IS_BIDIRECTIONAL = False\n",
    "# number of units (RNN_CELL ,nodes) in each layer\n",
    "UNITS = 128\n",
    "# dropout rate\n",
    "DROPOUT = 0.4\n",
    "### Training parameters\n",
    "LOSS = \"categorical_crossentropy\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 6\n",
    "\n",
    "def get_model_name(dataset_name):\n",
    "    # construct the unique model name\n",
    "    model_name = f\"{dataset_name}-{RNN_CELL.__name__}-seq-{SEQUENCE_LENGTH}-em-{EMBEDDING_SIZE}-w-{N_WORDS}-layers-{N_LAYERS}-units-{UNITS}-opt-{OPTIMIZER}-BS-{BATCH_SIZE}-d-{DROPOUT}\"\n",
    "    if IS_BIDIRECTIONAL:\n",
    "        # add 'bid' str if bidirectional\n",
    "        model_name = \"bid-\" + model_name\n",
    "    if OOV_TOKEN:\n",
    "        # add 'oov' str if OOV token is specified\n",
    "        model_name += \"-oov\"\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading GloVe: 400000it [00:24, 16045.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 300)          1151400   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 1,371,306\n",
      "Trainable params: 219,906\n",
      "Non-trainable params: 1,151,400\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      " 1/45 [..............................] - ETA: 0s - loss: 0.7012 - accuracy: 0.5312WARNING:tensorflow:From C:\\Users\\Ketaki Barde\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      " 2/45 [>.............................] - ETA: 44s - loss: 0.6203 - accuracy: 0.7109WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.5725s vs `on_train_batch_end` time: 1.4581s). Check your callbacks.\n",
      "45/45 [==============================] - 27s 593ms/step - loss: 0.2153 - accuracy: 0.9557 - val_loss: 0.1522 - val_accuracy: 0.9642\n",
      "Epoch 2/6\n",
      "45/45 [==============================] - 31s 685ms/step - loss: 0.1274 - accuracy: 0.9683 - val_loss: 0.1288 - val_accuracy: 0.9642\n",
      "Epoch 3/6\n",
      "45/45 [==============================] - 27s 608ms/step - loss: 0.1106 - accuracy: 0.9683 - val_loss: 0.1136 - val_accuracy: 0.9642\n",
      "Epoch 4/6\n",
      "45/45 [==============================] - 27s 595ms/step - loss: 0.0872 - accuracy: 0.9707 - val_loss: 0.1160 - val_accuracy: 0.9691\n",
      "Epoch 5/6\n",
      "45/45 [==============================] - 29s 655ms/step - loss: 0.0730 - accuracy: 0.9805 - val_loss: 0.0883 - val_accuracy: 0.9740\n",
      "Epoch 6/6\n",
      "45/45 [==============================] - 27s 592ms/step - loss: 0.0607 - accuracy: 0.9808 - val_loss: 0.1094 - val_accuracy: 0.9748\n"
     ]
    }
   ],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "# load the data\n",
    "data = data_preprocessing(N_WORDS, SEQUENCE_LENGTH, TEST_SIZE, oov_token=OOV_TOKEN)\n",
    "dataset_name = \"apache-ant\"\n",
    "model_name = get_model_name(dataset_name)\n",
    "\n",
    "# construct the model\n",
    "model = create_model(data[\"tokenizer\"].word_index, units=UNITS, n_layers=N_LAYERS, \n",
    "                    cell=RNN_CELL, bidirectional=IS_BIDIRECTIONAL, embedding_size=EMBEDDING_SIZE, \n",
    "                    sequence_length=SEQUENCE_LENGTH, dropout=DROPOUT, \n",
    "                    loss=LOSS, optimizer=OPTIMIZER, output_length=data[\"y_train\"][0].shape[0])\n",
    "model.summary()\n",
    "\n",
    "# using tensorboard on 'logs' folder\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# start training\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[tensorboard],\n",
    "                    verbose=1)\n",
    "# save the resulting model into 'results' folder\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(text):\n",
    "    sequence = data[\"tokenizer\"].texts_to_sequences([text])\n",
    "    # pad the sequences\n",
    "    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
    "    # get the prediction\n",
    "    prediction = model.predict(sequence)[0]\n",
    "    return prediction, data[\"int2label\"][np.argmax(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output vector: [0.9916232  0.00837673]\n",
      "Prediction: no\n"
     ]
    }
   ],
   "source": [
    "text = \"//DMS handle this well.\"\n",
    "output_vector, prediction = get_predictions(text)\n",
    "print(\"Output vector:\", output_vector)\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output vector: [0.36631143 0.63368857]\n",
      "Prediction: yes\n"
     ]
    }
   ],
   "source": [
    "text = \"// TODO fix the space above.\"\n",
    "output_vector, prediction = get_predictions(text)\n",
    "print(\"Output vector:\", output_vector)\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
