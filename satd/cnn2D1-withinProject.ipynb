{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "driving-companion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import nltk\\nnltk.download('stopwords')\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import glob\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics\n",
    "'''import nltk\n",
    "nltk.download('stopwords')'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "communist-magnet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "mental-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = list(glob.glob(\"data/*\"))\n",
    "dataframes = [pd.read_csv(file) for file in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "opening-staff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>projectname</th>\n",
       "      <th>classification</th>\n",
       "      <th>commenttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// FIXME formatters are not thread-safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// XXX: (Jon Skeet) The comment \"if it hasn't been done already\" may // not be strictly true. wrapper.maybeConfigure() won't configure the same // attributes/text more than once, but it may well add the children again, // unless I've missed something.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// I hate to admit it, but we don't know what happened // here.  Throw the Exception.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// Just a note: StarTeam has a status for NEW which implies // that there is an item  on your local machine that is not // in the repository.  These are the items that show up as // NOT IN VIEW in the Starteam GUI. // One would think that we would want to perhaps checkin the // NEW items (not in all cases! - Steve Cohen 15 Dec 2001) // Unfortunately, the sdk doesn't really work, and we can't // actually see  anything with a status of NEW. That is why // we can just check out  everything here without worrying // about losing anything.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// the generated classes must not be added in the generic JAR! // is that buggy on old JOnAS (2.4) ??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// todo: is this comment still relevant ?? // FIXME: need to use a SAXSource as the source for the transform // so we can plug in our own entity resolver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// This is turned off temporarily. There appears to be a bug // in SelectorUtils.matchPattern() where it is recursive on // Windows even if no ** is in pattern. //assertEquals(\"FFFTFFFFFFFF\", results); // Unix // vs //assertEquals(\"FFFTFFFFTFFF\", results); // Windows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// pattern now holds ** while string is not exhausted // this will generate false positives but we can live with that.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// FIXME: Is \"No Namespace is Empty Namespace\" really OK?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// FIXME this is actually not very cpu cycles friendly as we are converting from // dos to java while the underlying Sun implementation will convert // from java to dos time for internal storage...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        projectname classification  \\\n",
       "0  apache-ant-1.7.0         DEFECT   \n",
       "1  apache-ant-1.7.0         DEFECT   \n",
       "2  apache-ant-1.7.0         DEFECT   \n",
       "3  apache-ant-1.7.0         DEFECT   \n",
       "4  apache-ant-1.7.0         DEFECT   \n",
       "5  apache-ant-1.7.0         DEFECT   \n",
       "6  apache-ant-1.7.0         DEFECT   \n",
       "7  apache-ant-1.7.0         DEFECT   \n",
       "8  apache-ant-1.7.0         DEFECT   \n",
       "9  apache-ant-1.7.0         DEFECT   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   commenttext  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      // FIXME formatters are not thread-safe  \n",
       "1                                                                                                                                                                                                                                                                                                  // XXX: (Jon Skeet) The comment \"if it hasn't been done already\" may // not be strictly true. wrapper.maybeConfigure() won't configure the same // attributes/text more than once, but it may well add the children again, // unless I've missed something.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                        // I hate to admit it, but we don't know what happened // here.  Throw the Exception.  \n",
       "3  // Just a note: StarTeam has a status for NEW which implies // that there is an item  on your local machine that is not // in the repository.  These are the items that show up as // NOT IN VIEW in the Starteam GUI. // One would think that we would want to perhaps checkin the // NEW items (not in all cases! - Steve Cohen 15 Dec 2001) // Unfortunately, the sdk doesn't really work, and we can't // actually see  anything with a status of NEW. That is why // we can just check out  everything here without worrying // about losing anything.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                        // the generated classes must not be added in the generic JAR! // is that buggy on old JOnAS (2.4) ??  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                    // todo: is this comment still relevant ?? // FIXME: need to use a SAXSource as the source for the transform // so we can plug in our own entity resolver  \n",
       "6                                                                                                                                                                                                                                                                                  // This is turned off temporarily. There appears to be a bug // in SelectorUtils.matchPattern() where it is recursive on // Windows even if no ** is in pattern. //assertEquals(\"FFFTFFFFFFFF\", results); // Unix // vs //assertEquals(\"FFFTFFFFTFFF\", results); // Windows  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                       // pattern now holds ** while string is not exhausted // this will generate false positives but we can live with that.  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    // FIXME: Is \"No Namespace is Empty Namespace\" really OK?  \n",
       "9                                                                                                                                                                                                                                                                                                                                                        // FIXME this is actually not very cpu cycles friendly as we are converting from // dos to java while the underlying Sun implementation will convert // from java to dos time for internal storage...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.concat(dataframes)\n",
    "metadata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "satellite-vertical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>projectname</th>\n",
       "      <th>classification</th>\n",
       "      <th>commenttext</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// FIXME formatters are not thread-safe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// XXX: (Jon Skeet) The comment \"if it hasn't been done already\" may // not be strictly true. wrapper.maybeConfigure() won't configure the same // attributes/text more than once, but it may well add the children again, // unless I've missed something.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// I hate to admit it, but we don't know what happened // here.  Throw the Exception.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// Just a note: StarTeam has a status for NEW which implies // that there is an item  on your local machine that is not // in the repository.  These are the items that show up as // NOT IN VIEW in the Starteam GUI. // One would think that we would want to perhaps checkin the // NEW items (not in all cases! - Steve Cohen 15 Dec 2001) // Unfortunately, the sdk doesn't really work, and we can't // actually see  anything with a status of NEW. That is why // we can just check out  everything here without worrying // about losing anything.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apache-ant-1.7.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// the generated classes must not be added in the generic JAR! // is that buggy on old JOnAS (2.4) ??</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        projectname classification  \\\n",
       "0  apache-ant-1.7.0         DEFECT   \n",
       "1  apache-ant-1.7.0         DEFECT   \n",
       "2  apache-ant-1.7.0         DEFECT   \n",
       "3  apache-ant-1.7.0         DEFECT   \n",
       "4  apache-ant-1.7.0         DEFECT   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   commenttext  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      // FIXME formatters are not thread-safe   \n",
       "1                                                                                                                                                                                                                                                                                                  // XXX: (Jon Skeet) The comment \"if it hasn't been done already\" may // not be strictly true. wrapper.maybeConfigure() won't configure the same // attributes/text more than once, but it may well add the children again, // unless I've missed something.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                        // I hate to admit it, but we don't know what happened // here.  Throw the Exception.   \n",
       "3  // Just a note: StarTeam has a status for NEW which implies // that there is an item  on your local machine that is not // in the repository.  These are the items that show up as // NOT IN VIEW in the Starteam GUI. // One would think that we would want to perhaps checkin the // NEW items (not in all cases! - Steve Cohen 15 Dec 2001) // Unfortunately, the sdk doesn't really work, and we can't // actually see  anything with a status of NEW. That is why // we can just check out  everything here without worrying // about losing anything.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                        // the generated classes must not be added in the generic JAR! // is that buggy on old JOnAS (2.4) ??   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Form labels for SATD: 0 for non-satd and 1 for satd\n",
    "# Is important that text labels are converted into numeric for NN\n",
    "metadata['label'] = np.where(metadata.classification == 'WITHOUT_CLASSIFICATION', 0, 1)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "helpful-teens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    58204\n",
      "1     4071\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(metadata['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "musical-greenhouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apache-ant-1.7.0' 'apache-jmeter-2.10' 'argouml' 'columba-1.4-src'\n",
      " 'emf-2.4.1' 'hibernate-distribution-3.3.2.GA' 'jEdit-4.2'\n",
      " 'jfreechart-1.0.19' 'jruby-1.4.0' 'sql12']\n"
     ]
    }
   ],
   "source": [
    "print(metadata.projectname.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "enabling-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data_aug/argouml_aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "separate-jefferson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projectname       0\n",
      "classification    0\n",
      "commenttext       0\n",
      "label             0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4897 entries, 0 to 4896\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   projectname     4897 non-null   object\n",
      " 1   classification  4897 non-null   object\n",
      " 2   commenttext     4897 non-null   object\n",
      " 3   label           4897 non-null   int32 \n",
      "dtypes: int32(1), object(3)\n",
      "memory usage: 172.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#Specify in project dataset name to test\n",
    "df = metadata[metadata.projectname == 'jruby-1.4.0']\n",
    "print(df.isnull().sum())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "norman-permit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>projectname</th>\n",
       "      <th>classification</th>\n",
       "      <th>commenttext</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jruby-1.4.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// TODO: we loose encoding information here, fix it</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jruby-1.4.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>/* TODO: We should use implicit nil for body, but problem (punt til later)*/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jruby-1.4.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// TODO: Numeric.new works in Ruby and it does here too.  However trying to use //   that instance in a numeric operation should generate an ArgumentError. Doing //   this seems so pathological I do not see the need to fix this now.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jruby-1.4.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// TODO: Adding implicit nils caused multiple problems in compiler -- revist after landing //assert bodyNode != null : \"bodyNode is not null\";</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jruby-1.4.0</td>\n",
       "      <td>DEFECT</td>\n",
       "      <td>// TODO: take BigDecimal.mode into account.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   projectname classification  \\\n",
       "0  jruby-1.4.0         DEFECT   \n",
       "1  jruby-1.4.0         DEFECT   \n",
       "2  jruby-1.4.0         DEFECT   \n",
       "3  jruby-1.4.0         DEFECT   \n",
       "4  jruby-1.4.0         DEFECT   \n",
       "\n",
       "                                                                                                                                                                                                                                commenttext  \\\n",
       "0                                                                                                                                                                                       // TODO: we loose encoding information here, fix it   \n",
       "1                                                                                                                                                              /* TODO: We should use implicit nil for body, but problem (punt til later)*/   \n",
       "2  // TODO: Numeric.new works in Ruby and it does here too.  However trying to use //   that instance in a numeric operation should generate an ArgumentError. Doing //   this seems so pathological I do not see the need to fix this now.   \n",
       "3                                                                                            // TODO: Adding implicit nils caused multiple problems in compiler -- revist after landing //assert bodyNode != null : \"bodyNode is not null\";   \n",
       "4                                                                                                                                                                                               // TODO: take BigDecimal.mode into account.   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "phantom-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1] Preprocessing:\n",
    "1. Remove tags, punctuations, stop words, special characters and return X_clean and y as np arrays\n",
    "2. Split data in train and test. Remember Xclean and y should reflect metadata used for training\n",
    "def remove_tags(text):\n",
    "\n",
    "'''\n",
    "def remove_tags(text):\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "    return TAG_RE.sub('', text)\n",
    "    \n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "    \n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    \n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    \n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "    sentence = pattern.sub('', sentence)\n",
    "    #print(sentence)\n",
    "    return sentence\n",
    "\n",
    "def prepare_data(_df):\n",
    "    X_clean = []\n",
    "    sentences = list(_df['commenttext'])\n",
    "\n",
    "    for sen in sentences:\n",
    "        X_clean.append(preprocess_text(sen))\n",
    "\n",
    "    print(X_clean[:3], \"\\n\\n*************************\")\n",
    "    X_clean = np.array(X_clean)\n",
    "    y = np.array(_df.label)\n",
    "\n",
    "    print(\"Cleaned corpus shape:\", X_clean.shape, y.shape)\n",
    "    return X_clean, y\n",
    "\n",
    "def split_data(df):\n",
    "    X_clean, y = prepare_data(df)\n",
    "    print(X_clean[:3], \"\\n****kkk****\\n\")\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_clean, y, test_size=0.20, random_state=42)\n",
    "    print(\"Train set:\",X_train.shape, y_train.shape)\n",
    "    print(\"Validation set:\", X_val.shape, X_val.shape)\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "powered-behavior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' TODO loose encoding information fix ', ' TODO We use implicit nil body problem punt til later ', ' TODO Numeric new works Ruby However trying use instance numeric operation generate ArgumentError Doing seems pathological see need fix '] \n",
      "\n",
      "*************************\n",
      "Cleaned corpus shape: (4897,) (4897,)\n",
      "[' TODO loose encoding information fix '\n",
      " ' TODO We use implicit nil body problem punt til later '\n",
      " ' TODO Numeric new works Ruby However trying use instance numeric operation generate ArgumentError Doing seems pathological see need fix '] \n",
      "****kkk****\n",
      "\n",
      "Train set: (3917,) (3917,)\n",
      "Validation set: (980,) (980,)\n"
     ]
    }
   ],
   "source": [
    "#collect Xtrain and ytrain along with Xval and yval\n",
    "X_train, X_val, y_train, y_val = split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "earned-formation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training set head\n",
      "\n",
      "\n",
      "[' NOTE Currently optimization limited following situations All expressions must int ranged literal fixnums It also still emits code safe logic rather wasteful since essentially doubles code body As normally disabled serves example optimization could done Ideally combined processing improve code reuse generally available '\n",
      " ' line ' ' octal constant ' ... ' lists aggregate variables bodies whens'\n",
      " ' support encodings' ' nothing']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nTraining set head\\n\\n\")\n",
    "print((X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "broke-average",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3917,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hundred-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2] Tokenize the splitted data and convert them intp sequences and add post padding.\n",
    "Final output stored as X_train_padded, X-val_padded and y_train and y_val\n",
    "\n",
    "'''\n",
    "\n",
    "def create_features(X_train, X_val, y_train, y_val):\n",
    "    \n",
    "    #Step 2: Use keras to tokenize words and find word.index'length for getting number of unique words i.e vocab size\n",
    "\n",
    "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                          lower=True)\n",
    "\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print('VOCAB SIZE: Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    # Step 3: Embed the sentences into numbers using text to sequences\n",
    "\n",
    "    sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "    sequences_valid = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "    # Step 4: next step is to find the number of words in the longest sentence and then to apply \n",
    "    #padding to the sentences having shorter lengths than the length of the longest sentence\n",
    "\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "    longest_sentence = max(X_train, key=word_count)\n",
    "    length_long_sentence = len(word_tokenize(longest_sentence))\n",
    "    print(\"Longest sentence length: \",  length_long_sentence)\n",
    "    \n",
    "    print(\"**********************\")\n",
    "    \n",
    "    #Step 5: Pad zeros at the end using the length of the longest word \n",
    "    X_train_padded = pad_sequences(sequences_train,length_long_sentence, padding='post')\n",
    "    X_val_padded = pad_sequences(sequences_valid,padding='post', maxlen = length_long_sentence)\n",
    "\n",
    "\n",
    "    print('Shape of X train and X validation tensor:', X_train_padded.shape,X_val_padded.shape)\n",
    "    print('Shape of label train and validation tensor:', y_train.shape,y_val.shape)\n",
    "    \n",
    "    return X_train_padded, X_val_padded, y_train, y_val, vocab_size, word_index, length_long_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "alleged-hypothetical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE: Found 3318 unique tokens.\n",
      "Longest sentence length:  109\n",
      "**********************\n",
      "Shape of X train and X validation tensor: (3917, 109) (980, 109)\n",
      "Shape of label train and validation tensor: (3917,) (980,)\n"
     ]
    }
   ],
   "source": [
    "X_train_padded, X_val_padded, y_train, y_val, vocab_size, word_index, length_long_sentence= create_features(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "passive-rings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  91  333  304 ...    0    0    0]\n",
      " [   1    0    0 ...    0    0    0]\n",
      " [1332  127    0 ...    0    0    0]\n",
      " ...\n",
      " [3316 1539   77 ...    0    0    0]\n",
      " [ 188 3318    0 ...    0    0    0]\n",
      " [ 136    0    0 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "independent-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''3] Build embedidng matrix : Word2vec and glove (needs fixes)'''\n",
    "def build_embedding_matrix_word2vec(vocab_size, word_index):\n",
    "    import gensim\n",
    "    from gensim.models import Word2Vec\n",
    "    from gensim.utils import simple_preprocess\n",
    "\n",
    "    from gensim.models.keyedvectors import KeyedVectors\n",
    "    path_to_word2vec = 'E:/RIT/GA-TECHNICAL DEBTS/cnn-satd-ketaki/pretrained-embeddings/fasttext/crawl-300d-2M-subword.vec'\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(path_to_word2vec)\n",
    "    \n",
    "    # max unique words to keep\n",
    "    # define matrix dimensions\n",
    "    EMBEDDING_DIM= 300\n",
    "    vocab_size = vocab_size\n",
    "    \n",
    "    #Size of embed matrix must be = vocab sizeV * dimension of embeding\n",
    "    #Step 6: Generate an embedding matrix to get embeddings representaion of words in our corpus\n",
    "    #Our embedding_matrix now contains pretrained word embeddings for the words in our corpus.\n",
    "    embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i>=vocab_size:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = word_vectors[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "    \n",
    "    return embedding_matrix, word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "verbal-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, word_vectors =  build_embedding_matrix_word2vec(vocab_size, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-marriage",
   "metadata": {},
   "source": [
    "# Embeddings analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "entitled-twins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Embedding matrix shape: (3319, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nEmbedding matrix shape:\",embedding_matrix.shape)\n",
    "#embeddingMatrix[524] gives embedidngs i.e array of 300 vals\n",
    "#524 is index of word number 524 from our seq array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "curious-leeds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TODO', 0.7365497350692749),\n",
       " ('TODOs', 0.6493289470672607),\n",
       " ('ifdef', 0.6315240859985352),\n",
       " ('fixme', 0.6227824687957764),\n",
       " ('ifdefs', 0.6155142784118652),\n",
       " ('japhb', 0.6077137589454651),\n",
       " ('pmurias', 0.6055840253829956),\n",
       " ('kfree', 0.6007481217384338),\n",
       " ('hixie', 0.5931932926177979),\n",
       " ('PerlJam', 0.5903174877166748)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"FIXME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "contained-hacker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TODO', 0.9392647743225098),\n",
       " ('FIXME', 0.9239456057548523),\n",
       " ('TODOs', 0.7614597082138062),\n",
       " ('VTODO', 0.6649309396743774),\n",
       " ('ifdef', 0.6403259038925171),\n",
       " ('fixme', 0.6384193301200867),\n",
       " ('ifdefs', 0.6251087188720703),\n",
       " ('README', 0.6194640398025513),\n",
       " ('ChangeLog', 0.6060833930969238),\n",
       " ('pmurias', 0.606055736541748)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get vectors for the words\n",
    "todo = word_vectors.get_vector(\"TODO\")\n",
    "fixme = word_vectors.get_vector(\"FIXME\")\n",
    "# calculate and get closest match for resulting vector\n",
    "word_vectors.similar_by_vector(fixme + todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "light-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''4] Weighted loss func to handle class imbalance'''\n",
    "def sklearn_weighted_loss(y_train):\n",
    "    # Calculate the weights for each class so that we can balance the data\n",
    "    #The minority class will have a higher class weight\n",
    "    from sklearn.utils import class_weight\n",
    "    weights = class_weight.compute_class_weight('balanced',\n",
    "                                                np.unique(y_train),\n",
    "                                                y_train)\n",
    "    print(\"Resultant weight:\", weights)\n",
    "    sklearn_weight = {0: weights[0], 1: weights[1]}\n",
    "    return sklearn_weight\n",
    "\n",
    "def weighted_loss(y_train):\n",
    "    '''Link: https://www.tensorflow.org/tutorials/structured_data/imbalanced_data'''\n",
    "\n",
    "    neg, pos = np.bincount(y_train)\n",
    "    total = neg + pos\n",
    "    # Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "    # The sum of the weights of all examples stays the same.\n",
    "    weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "    weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "    print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "    print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "    return class_weight\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "metric-grace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for class 0: 0.57\n",
      "Weight for class 1: 4.07\n",
      "Custom weights: {0: 0.5699941792782305, 1: 4.071725571725572}\n",
      "Resultant weight: [0.56999418 4.07172557]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ketak\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 0 0 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "custom_class_weights = weighted_loss(y_train)\n",
    "print(\"Custom weights:\", custom_class_weights)\n",
    "#or use \n",
    "sklearn_weights= sklearn_weighted_loss(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adapted-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from tensorflow.keras.layers import Reshape, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend \n",
    "EMBEDDING_DIM = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-soundtrack",
   "metadata": {},
   "source": [
    "# 2D CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "demographic-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try window size of 1\n",
    "#(i am underfitting, try ways to improve)\n",
    "#3conv layer, 1.) add DENSE LAYERS before\n",
    "\n",
    "#2) repeat conv layers * 3, padding options to keep o/p size same\n",
    "#try to overfit first, train loss decreases, val loss increases\n",
    "\n",
    "#remove batch norm, drop and regu because those are for overfit. you are underfit so try complex model, more data, augment data\n",
    "'''Questions for DR. Yu:\n",
    "1. Should I add dense layers in b/w conv layers? \n",
    "2. How many units if neurons in dense layers is recommended? Also, ideally how many epochs to start and compare perf?\n",
    "3. Ideal values for loss look like? 0.1?\n",
    "4. Goal is to OVERFIT and then regularize, batch norm and increase F1.\n",
    "5. Match results with that of the paper and try on corrected data?\n",
    "5. What exactly did we do to correct this data?\n",
    "6. Applied DS course plan\n",
    "7. Run on RC?\n",
    "\n",
    "later\n",
    "maybe rnn is good eg\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''Vinay notes:\n",
    "change filter sizes array to be [1*WordDim, 2*WordDim...] so output is 1*n\n",
    "and then number of filters * filtersizes[i]\n",
    "then do maxpool only on (filtersize,1)\n",
    "hgram matrix 163*300 put h gram matrix on this 162*300 aise 4 matrics 162*300*4 then put filter matrix 162*4\n",
    "hgram = [162, 163, 163]\n",
    "'''\n",
    "filter_sizes = [1,2,3,4,5]\n",
    "num_filters = 64\n",
    "drop = 0.1\n",
    "EMBEDDING_DIM = 300 \n",
    "\n",
    "deep_inputs = Input(shape=(length_long_sentence,))\n",
    "embedding = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)(deep_inputs)\n",
    "reshape = Reshape((length_long_sentence,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu', kernel_regularizer = regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu', kernel_regularizer = regularizers.l2(0.01))(reshape)\n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu', kernel_regularizer = regularizers.l2(0.01))(reshape)\n",
    "conv_3 = Conv2D(num_filters, (filter_sizes[3], EMBEDDING_DIM),activation='relu', kernel_regularizer = regularizers.l2(0.01))(reshape)\n",
    "conv_4 = Conv2D(num_filters, (filter_sizes[4], EMBEDDING_DIM),activation='relu', kernel_regularizer = regularizers.l2(0.01))(reshape)\n",
    "maxpool_0 = MaxPooling2D((length_long_sentence - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((length_long_sentence - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((length_long_sentence - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "maxpool_3 = MaxPooling2D((length_long_sentence - filter_sizes[3] + 1, 1), strides=(1,1))(conv_3)\n",
    "maxpool_4 = MaxPooling2D((length_long_sentence - filter_sizes[4] + 1, 1), strides=(1,1))(conv_4)\n",
    "\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1,maxpool_2, maxpool_3, maxpool_4], axis=3)\n",
    "\n",
    "#batch_normalization = tf.keras.layers.BatchNormalization()(merged_tensor)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "#reshape = Reshape((5*num_filters,))(flatten)\n",
    "#dropout = Dropout(drop)(flatten)\n",
    "\n",
    "#add dense layers here, RELU same i/p,  o/p size\n",
    "cov0_dense =  Dense(units=64, activation='relu', kernel_regularizer = regularizers.l2(0.01))(flatten)\n",
    "cov1_dense =  Dense(units=64, activation='relu', kernel_regularizer = regularizers.l2(0.01))(cov0_dense)\n",
    "cov2_dense =  Dense(units=64, activation='relu', kernel_regularizer = regularizers.l2(0.01))(cov1_dense)\n",
    "dense_1 =  Dense(units=64, activation='relu', kernel_regularizer = regularizers.l2(0.01))(cov2_dense)\n",
    "\n",
    "output = Dense(units=1, activation='sigmoid', kernel_regularizer = regularizers.l2(0.01))(dense_1)\n",
    "#p(c=1)+ p(c=0) = 1\n",
    "# this creates a model that includes\n",
    "cnn2d_model = Model(deep_inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "strong-acoustic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 109)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 109, 300)     995700      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 109, 300, 1)  0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 109, 1, 64)   19264       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 108, 1, 64)   38464       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 107, 1, 64)   57664       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 106, 1, 64)   76864       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 105, 1, 64)   96064       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 64)     0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 64)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 64)     0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 64)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 64)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1, 1, 320)    0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 320)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           20544       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           4160        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           4160        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           4160        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            65          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,317,109\n",
      "Trainable params: 321,409\n",
      "Non-trainable params: 995,700\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#adam = Adam(lr=0.003)\n",
    "sgd = SGD(learning_rate=0.0095)\n",
    "cnn2d_model.compile(optimizer = sgd , loss='binary_crossentropy', metrics=['accuracy']) \n",
    "#use tf.keras.losses.BinaryCrossentropy(from_logits=True) in loss\n",
    "callbacks = [EarlyStopping(monitor='val_loss',  min_delta=0.000001)]\n",
    "print(cnn2d_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "surprising-scott",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "53/53 [==============================] - 3s 58ms/step - loss: 3.7258 - accuracy: 0.8567 - val_loss: 3.6409 - val_accuracy: 0.8895\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 3s 53ms/step - loss: 3.5808 - accuracy: 0.8750 - val_loss: 3.5075 - val_accuracy: 0.8895\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 3s 56ms/step - loss: 3.4621 - accuracy: 0.8750 - val_loss: 3.3963 - val_accuracy: 0.8895\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 3s 54ms/step - loss: 3.3622 - accuracy: 0.8750 - val_loss: 3.3014 - val_accuracy: 0.8895\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 3.2764 - accuracy: 0.8750 - val_loss: 3.2191 - val_accuracy: 0.8895\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 3s 59ms/step - loss: 3.2012 - accuracy: 0.8750 - val_loss: 3.1462 - val_accuracy: 0.8895\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 3.1338 - accuracy: 0.8750 - val_loss: 3.0803 - val_accuracy: 0.8895\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 3s 55ms/step - loss: 3.0720 - accuracy: 0.8750 - val_loss: 3.0194 - val_accuracy: 0.8895\n",
      "Epoch 9/50\n",
      "38/53 [====================>.........] - ETA: 0s - loss: 3.0234 - accuracy: 0.8738"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-b5d51aa21f00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#try wihtout weighted loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m cnn2d_history = cnn2d_model.fit(X_train_padded, y_train, batch_size=64, epochs=50, validation_split=0.15, \n\u001b[0m\u001b[0;32m      3\u001b[0m                                 shuffle = False )\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#rmsprop or zyada fast wala read\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m     \"\"\"\n\u001b[1;32m-> 1661\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    591\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#try wihtout weighted loss\n",
    "cnn2d_history = cnn2d_model.fit(X_train_padded, y_train, batch_size=64, epochs=50, validation_split=0.15, \n",
    "                                shuffle = False )\n",
    "#rmsprop or zyada fast wala read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "stupid-intranet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 8ms/step - loss: 1.5287 - accuracy: 0.8561\n",
      "Test Loss: 1.528658151626587\n",
      "Test Accuracy: 0.8561224341392517\n"
     ]
    }
   ],
   "source": [
    "score = cnn2d_model.evaluate(X_val_padded, y_val, verbose=1)\n",
    "print(\"Test Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "guided-boundary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnn2d_history.history['accuracy'])\n",
    "plt.plot(cnn2d_history.history['val_accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.ylim([0,1.01])\n",
    "plt.show()\n",
    "plt.savefig('AccEpoch24_2.png')\n",
    "plt.plot(cnn2d_history.history['loss'])\n",
    "plt.plot(cnn2d_history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig('LossEpoch24_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "informative-lindsay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(\"Precision:\", round(precision_score(y_val, y_pred , average=\"macro\"),2))\\nprint(\"Recall:\", round(recall_score(y_val, y_pred, average=\"macro\"),2))\\nprint(\"F1 Score:\", round(f1_score(y_val, y_pred , average=\"macro\"),2))\\n'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "y_pred1 = cnn2d_model.predict(X_val_padded)\n",
    "\n",
    "y_pred = np.argmax(y_pred1, axis=1) #this is if it was softmax\n",
    "\n",
    "#Sigmoid if > 0.5 eg. \n",
    "\n",
    "# Print f1, precision, and recall scores\n",
    "'''print(\"Precision:\", round(precision_score(y_val, y_pred , average=\"macro\"),2))\n",
    "print(\"Recall:\", round(recall_score(y_val, y_pred, average=\"macro\"),2))\n",
    "print(\"F1 Score:\", round(f1_score(y_val, y_pred , average=\"macro\"),2))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-pencil",
   "metadata": {},
   "source": [
    "# Sigmoid gives one ypred, if its val <0.5 class 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "orange-craps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "#we are basically take satd label f1 as pos labe for satd is 1\n",
    "ypred_ = [1 if pred[0]>0.5 else 0 for pred in y_pred1]\n",
    "print(\"F1 Score:\", round(f1_score(y_val, ypred_ , average=\"binary\", pos_label = 1),2)) #pos label is SATD Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "impossible-thunder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ketak\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\", round(precision_score(y_val, ypred_ , average=\"binary\", pos_label = 1),2))\n",
    "print(\"Recall:\", round(recall_score(y_val, ypred_, average=\"binary\", pos_label = 1),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-things",
   "metadata": {},
   "source": [
    "* 1 Sentence => 1 Matrix of size : #words x wD\n",
    "- N sentences => N matrix of above size \n",
    "- every sentence has different words of differnt size\n",
    "- sort the sentences in increasing length of words and truncate the sentences above 95-percentile length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "behavioral-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.clear_session()\n",
    "#  tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "reflected-nightlife",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15895101],\n",
       "       [0.17074144],\n",
       "       [0.12012801],\n",
       "       [0.1230244 ],\n",
       "       [0.11078948],\n",
       "       [0.13207343],\n",
       "       [0.09737736],\n",
       "       [0.09587324],\n",
       "       [0.1666744 ],\n",
       "       [0.11422768],\n",
       "       [0.10446528],\n",
       "       [0.09587324],\n",
       "       [0.11898252],\n",
       "       [0.10438353],\n",
       "       [0.11251342],\n",
       "       [0.1084893 ],\n",
       "       [0.12207875],\n",
       "       [0.11220005],\n",
       "       [0.13365111],\n",
       "       [0.11154383],\n",
       "       [0.12655333],\n",
       "       [0.11315262],\n",
       "       [0.10024768],\n",
       "       [0.1663039 ],\n",
       "       [0.09587324],\n",
       "       [0.12879261],\n",
       "       [0.09587324],\n",
       "       [0.11510307],\n",
       "       [0.10771438],\n",
       "       [0.15686545],\n",
       "       [0.09587324],\n",
       "       [0.09931418],\n",
       "       [0.10510781],\n",
       "       [0.10269421],\n",
       "       [0.1673595 ],\n",
       "       [0.11074418],\n",
       "       [0.11220005],\n",
       "       [0.10430074],\n",
       "       [0.12375361],\n",
       "       [0.10440239],\n",
       "       [0.09587324],\n",
       "       [0.12794012],\n",
       "       [0.13047904],\n",
       "       [0.11649883],\n",
       "       [0.11647743],\n",
       "       [0.09587324],\n",
       "       [0.11625451],\n",
       "       [0.16649112],\n",
       "       [0.12935331],\n",
       "       [0.1053012 ],\n",
       "       [0.11481109],\n",
       "       [0.09538922],\n",
       "       [0.12283379],\n",
       "       [0.11441079],\n",
       "       [0.12886709],\n",
       "       [0.09898877],\n",
       "       [0.10024768],\n",
       "       [0.15006316],\n",
       "       [0.16099977],\n",
       "       [0.11525995],\n",
       "       [0.10519353],\n",
       "       [0.11195835],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.16466457],\n",
       "       [0.17095569],\n",
       "       [0.09587324],\n",
       "       [0.11453164],\n",
       "       [0.11744148],\n",
       "       [0.10885248],\n",
       "       [0.12596482],\n",
       "       [0.10035342],\n",
       "       [0.12779427],\n",
       "       [0.11231118],\n",
       "       [0.11011928],\n",
       "       [0.09587324],\n",
       "       [0.08987272],\n",
       "       [0.12326971],\n",
       "       [0.10396749],\n",
       "       [0.15781844],\n",
       "       [0.15255445],\n",
       "       [0.12121087],\n",
       "       [0.11603284],\n",
       "       [0.11081782],\n",
       "       [0.1173653 ],\n",
       "       [0.11849874],\n",
       "       [0.10024768],\n",
       "       [0.12652937],\n",
       "       [0.13066205],\n",
       "       [0.13470426],\n",
       "       [0.09587324],\n",
       "       [0.10263193],\n",
       "       [0.09587324],\n",
       "       [0.10223991],\n",
       "       [0.15565887],\n",
       "       [0.09587324],\n",
       "       [0.11056867],\n",
       "       [0.09587324],\n",
       "       [0.13563076],\n",
       "       [0.1594083 ],\n",
       "       [0.10306326],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.10003752],\n",
       "       [0.09587324],\n",
       "       [0.17074144],\n",
       "       [0.10341531],\n",
       "       [0.09587324],\n",
       "       [0.17074144],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.10786048],\n",
       "       [0.10454133],\n",
       "       [0.12127218],\n",
       "       [0.12595865],\n",
       "       [0.09587324],\n",
       "       [0.11639178],\n",
       "       [0.17074144],\n",
       "       [0.10260296],\n",
       "       [0.12009868],\n",
       "       [0.1634382 ],\n",
       "       [0.1330018 ],\n",
       "       [0.13554373],\n",
       "       [0.1470151 ],\n",
       "       [0.12537444],\n",
       "       [0.11969736],\n",
       "       [0.10814488],\n",
       "       [0.10765624],\n",
       "       [0.10752007],\n",
       "       [0.11084068],\n",
       "       [0.1123296 ],\n",
       "       [0.09587324],\n",
       "       [0.15895215],\n",
       "       [0.12447163],\n",
       "       [0.12426969],\n",
       "       [0.11723396],\n",
       "       [0.13224396],\n",
       "       [0.12208918],\n",
       "       [0.09587324],\n",
       "       [0.13371545],\n",
       "       [0.12538421],\n",
       "       [0.12151167],\n",
       "       [0.1139324 ],\n",
       "       [0.12716225],\n",
       "       [0.12514368],\n",
       "       [0.09587324],\n",
       "       [0.10561115],\n",
       "       [0.09587324],\n",
       "       [0.12591806],\n",
       "       [0.10385385],\n",
       "       [0.11293983],\n",
       "       [0.12879261],\n",
       "       [0.11412063],\n",
       "       [0.15087074],\n",
       "       [0.10113698],\n",
       "       [0.16854262],\n",
       "       [0.12068227],\n",
       "       [0.10828635],\n",
       "       [0.09587324],\n",
       "       [0.12037724],\n",
       "       [0.10413778],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.11049059],\n",
       "       [0.17074144],\n",
       "       [0.1735403 ],\n",
       "       [0.10272422],\n",
       "       [0.11246446],\n",
       "       [0.11885488],\n",
       "       [0.0994257 ],\n",
       "       [0.12473917],\n",
       "       [0.12643254],\n",
       "       [0.15534654],\n",
       "       [0.12056404],\n",
       "       [0.11714202],\n",
       "       [0.09999004],\n",
       "       [0.12217987],\n",
       "       [0.10512003],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.1324605 ],\n",
       "       [0.11671433],\n",
       "       [0.09587324],\n",
       "       [0.1211772 ],\n",
       "       [0.11484203],\n",
       "       [0.11128733],\n",
       "       [0.14563388],\n",
       "       [0.12159461],\n",
       "       [0.11006838],\n",
       "       [0.09587324],\n",
       "       [0.10659188],\n",
       "       [0.12287501],\n",
       "       [0.11580133],\n",
       "       [0.11210373],\n",
       "       [0.10435492],\n",
       "       [0.15292257],\n",
       "       [0.15917313],\n",
       "       [0.12139013],\n",
       "       [0.17247114],\n",
       "       [0.11604953],\n",
       "       [0.12727088],\n",
       "       [0.09898877],\n",
       "       [0.11585292],\n",
       "       [0.11073691],\n",
       "       [0.13193616],\n",
       "       [0.1631976 ],\n",
       "       [0.0994235 ],\n",
       "       [0.12044004],\n",
       "       [0.09537566],\n",
       "       [0.11903071],\n",
       "       [0.11075675],\n",
       "       [0.09587324],\n",
       "       [0.1068674 ],\n",
       "       [0.11560023],\n",
       "       [0.11308295],\n",
       "       [0.1661464 ],\n",
       "       [0.13140556],\n",
       "       [0.09587324],\n",
       "       [0.12636957],\n",
       "       [0.13731655],\n",
       "       [0.09587324],\n",
       "       [0.11430806],\n",
       "       [0.1127032 ],\n",
       "       [0.11966774],\n",
       "       [0.09587324],\n",
       "       [0.15326723],\n",
       "       [0.12317175],\n",
       "       [0.10930544],\n",
       "       [0.12747836],\n",
       "       [0.0994235 ],\n",
       "       [0.10334411],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.10277265],\n",
       "       [0.15900055],\n",
       "       [0.11525312],\n",
       "       [0.11941928],\n",
       "       [0.16469362],\n",
       "       [0.10212344],\n",
       "       [0.09587324],\n",
       "       [0.16463888],\n",
       "       [0.12147206],\n",
       "       [0.11180818],\n",
       "       [0.1213451 ],\n",
       "       [0.15970871],\n",
       "       [0.11895332],\n",
       "       [0.0994235 ],\n",
       "       [0.12409604],\n",
       "       [0.12051055],\n",
       "       [0.1046834 ],\n",
       "       [0.09587324],\n",
       "       [0.13554373],\n",
       "       [0.1154463 ],\n",
       "       [0.09966981],\n",
       "       [0.10998338],\n",
       "       [0.09869793],\n",
       "       [0.10024768],\n",
       "       [0.09587324],\n",
       "       [0.11806601],\n",
       "       [0.1675787 ],\n",
       "       [0.10024768],\n",
       "       [0.10670897],\n",
       "       [0.09992284],\n",
       "       [0.10476407],\n",
       "       [0.10667062],\n",
       "       [0.10024768],\n",
       "       [0.15481815],\n",
       "       [0.12470928],\n",
       "       [0.15745357],\n",
       "       [0.09587324],\n",
       "       [0.17074144],\n",
       "       [0.10709661],\n",
       "       [0.17199183],\n",
       "       [0.12380767],\n",
       "       [0.11409557],\n",
       "       [0.14815876],\n",
       "       [0.12058184],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.10813329],\n",
       "       [0.09587324],\n",
       "       [0.1608617 ],\n",
       "       [0.1211772 ],\n",
       "       [0.11333525],\n",
       "       [0.1097309 ],\n",
       "       [0.10382417],\n",
       "       [0.09587324],\n",
       "       [0.11941969],\n",
       "       [0.11124659],\n",
       "       [0.09587324],\n",
       "       [0.10401893],\n",
       "       [0.1251618 ],\n",
       "       [0.09674007],\n",
       "       [0.17074144],\n",
       "       [0.10396749],\n",
       "       [0.10743636],\n",
       "       [0.11195835],\n",
       "       [0.1137237 ],\n",
       "       [0.09538922],\n",
       "       [0.1023629 ],\n",
       "       [0.13440198],\n",
       "       [0.10385841],\n",
       "       [0.13073477],\n",
       "       [0.1113061 ],\n",
       "       [0.11404377],\n",
       "       [0.12391064],\n",
       "       [0.12748295],\n",
       "       [0.11517456],\n",
       "       [0.10501155],\n",
       "       [0.12642992],\n",
       "       [0.11632484],\n",
       "       [0.1139324 ],\n",
       "       [0.11690331],\n",
       "       [0.11625659],\n",
       "       [0.10024768],\n",
       "       [0.10179242],\n",
       "       [0.11493587],\n",
       "       [0.09587324],\n",
       "       [0.11011928],\n",
       "       [0.11492574],\n",
       "       [0.09587324],\n",
       "       [0.14653581],\n",
       "       [0.10615417],\n",
       "       [0.12273702],\n",
       "       [0.09966981],\n",
       "       [0.11929408],\n",
       "       [0.11682987],\n",
       "       [0.10840559],\n",
       "       [0.10291871],\n",
       "       [0.10024768],\n",
       "       [0.17074144],\n",
       "       [0.15099722],\n",
       "       [0.12026575],\n",
       "       [0.12218264],\n",
       "       [0.15673366],\n",
       "       [0.11978602],\n",
       "       [0.09587324],\n",
       "       [0.12476277],\n",
       "       [0.11285356],\n",
       "       [0.10319471],\n",
       "       [0.10024768],\n",
       "       [0.11220509],\n",
       "       [0.11120099],\n",
       "       [0.11105034],\n",
       "       [0.12246689],\n",
       "       [0.15917313],\n",
       "       [0.10950047],\n",
       "       [0.12736574],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.1045287 ],\n",
       "       [0.11714202],\n",
       "       [0.09587324],\n",
       "       [0.11176535],\n",
       "       [0.12038183],\n",
       "       [0.09587324],\n",
       "       [0.17074144],\n",
       "       [0.09587324],\n",
       "       [0.14860997],\n",
       "       [0.16856852],\n",
       "       [0.10533309],\n",
       "       [0.17087263],\n",
       "       [0.113276  ],\n",
       "       [0.09974825],\n",
       "       [0.09992284],\n",
       "       [0.12141407],\n",
       "       [0.09538922],\n",
       "       [0.1267274 ],\n",
       "       [0.11226279],\n",
       "       [0.10024768],\n",
       "       [0.11288667],\n",
       "       [0.11920062],\n",
       "       [0.11287126],\n",
       "       [0.12655333],\n",
       "       [0.09114039],\n",
       "       [0.11932105],\n",
       "       [0.1663039 ],\n",
       "       [0.11011037],\n",
       "       [0.12141988],\n",
       "       [0.10024768],\n",
       "       [0.11078012],\n",
       "       [0.17074189],\n",
       "       [0.09968936],\n",
       "       [0.10953125],\n",
       "       [0.12868133],\n",
       "       [0.10828096],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.11183241],\n",
       "       [0.1390323 ],\n",
       "       [0.15736353],\n",
       "       [0.09587324],\n",
       "       [0.10366389],\n",
       "       [0.11758971],\n",
       "       [0.09587324],\n",
       "       [0.12580433],\n",
       "       [0.12074754],\n",
       "       [0.10469535],\n",
       "       [0.10825875],\n",
       "       [0.10886028],\n",
       "       [0.12003237],\n",
       "       [0.1663039 ],\n",
       "       [0.10981202],\n",
       "       [0.11680573],\n",
       "       [0.11858788],\n",
       "       [0.09945777],\n",
       "       [0.09587324],\n",
       "       [0.10119751],\n",
       "       [0.1280689 ],\n",
       "       [0.10481051],\n",
       "       [0.1574404 ],\n",
       "       [0.10717377],\n",
       "       [0.16050118],\n",
       "       [0.1246089 ],\n",
       "       [0.15534654],\n",
       "       [0.11265638],\n",
       "       [0.09587324],\n",
       "       [0.11731616],\n",
       "       [0.12303361],\n",
       "       [0.17074144],\n",
       "       [0.09587324],\n",
       "       [0.1278773 ],\n",
       "       [0.1148189 ],\n",
       "       [0.11195862],\n",
       "       [0.13231558],\n",
       "       [0.15854171],\n",
       "       [0.12745932],\n",
       "       [0.17074144],\n",
       "       [0.16842097],\n",
       "       [0.12162489],\n",
       "       [0.10055816],\n",
       "       [0.11070514],\n",
       "       [0.12299827],\n",
       "       [0.1006608 ],\n",
       "       [0.12974364],\n",
       "       [0.13284525],\n",
       "       [0.12546512],\n",
       "       [0.11912769],\n",
       "       [0.09587324],\n",
       "       [0.11610842],\n",
       "       [0.12087741],\n",
       "       [0.09505835],\n",
       "       [0.12185758],\n",
       "       [0.09587324],\n",
       "       [0.16677219],\n",
       "       [0.14126655],\n",
       "       [0.09587324],\n",
       "       [0.11359838],\n",
       "       [0.11936212],\n",
       "       [0.10818869],\n",
       "       [0.13434407],\n",
       "       [0.11934593],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.10024768],\n",
       "       [0.11107418],\n",
       "       [0.09587324],\n",
       "       [0.11627424],\n",
       "       [0.14815876],\n",
       "       [0.12221971],\n",
       "       [0.12773585],\n",
       "       [0.15493977],\n",
       "       [0.08659163],\n",
       "       [0.11982003],\n",
       "       [0.11511335],\n",
       "       [0.12156034],\n",
       "       [0.17074144],\n",
       "       [0.17317107],\n",
       "       [0.09587324],\n",
       "       [0.10731798],\n",
       "       [0.12679252],\n",
       "       [0.1258257 ],\n",
       "       [0.12231529],\n",
       "       [0.10445601],\n",
       "       [0.10589209],\n",
       "       [0.1044223 ],\n",
       "       [0.16975424],\n",
       "       [0.09994254],\n",
       "       [0.11255687],\n",
       "       [0.15989175],\n",
       "       [0.11632484],\n",
       "       [0.13496763],\n",
       "       [0.09898877],\n",
       "       [0.12700453],\n",
       "       [0.13549936],\n",
       "       [0.09737736],\n",
       "       [0.0994235 ],\n",
       "       [0.11233383],\n",
       "       [0.13606957],\n",
       "       [0.12246689],\n",
       "       [0.17074144],\n",
       "       [0.16319159],\n",
       "       [0.12056404],\n",
       "       [0.15271741],\n",
       "       [0.11465794],\n",
       "       [0.10337886],\n",
       "       [0.12747836],\n",
       "       [0.1092608 ],\n",
       "       [0.1369713 ],\n",
       "       [0.15121093],\n",
       "       [0.11081782],\n",
       "       [0.1139324 ],\n",
       "       [0.10024768],\n",
       "       [0.09587324],\n",
       "       [0.1346145 ],\n",
       "       [0.1225628 ],\n",
       "       [0.09587324],\n",
       "       [0.11655155],\n",
       "       [0.0994235 ],\n",
       "       [0.12177974],\n",
       "       [0.13391522],\n",
       "       [0.09114039],\n",
       "       [0.11108309],\n",
       "       [0.10762385],\n",
       "       [0.1705015 ],\n",
       "       [0.11932606],\n",
       "       [0.11598569],\n",
       "       [0.10024768],\n",
       "       [0.12061355],\n",
       "       [0.11335537],\n",
       "       [0.09587324],\n",
       "       [0.16059485],\n",
       "       [0.12379718],\n",
       "       [0.11720341],\n",
       "       [0.14975315],\n",
       "       [0.10159895],\n",
       "       [0.12348124],\n",
       "       [0.1574322 ],\n",
       "       [0.11570403],\n",
       "       [0.17029929],\n",
       "       [0.12552407],\n",
       "       [0.17074144],\n",
       "       [0.09931418],\n",
       "       [0.10190949],\n",
       "       [0.11597088],\n",
       "       [0.12084347],\n",
       "       [0.09966981],\n",
       "       [0.09587324],\n",
       "       [0.16072753],\n",
       "       [0.12010461],\n",
       "       [0.10894689],\n",
       "       [0.10024768],\n",
       "       [0.09587324],\n",
       "       [0.13418156],\n",
       "       [0.09966981],\n",
       "       [0.10025439],\n",
       "       [0.11394969],\n",
       "       [0.10840702],\n",
       "       [0.09587324],\n",
       "       [0.09674007],\n",
       "       [0.10527149],\n",
       "       [0.10003752],\n",
       "       [0.1337544 ],\n",
       "       [0.12031785],\n",
       "       [0.12958822],\n",
       "       [0.11292005],\n",
       "       [0.13904908],\n",
       "       [0.12044004],\n",
       "       [0.12493014],\n",
       "       [0.11975804],\n",
       "       [0.10382417],\n",
       "       [0.09587324],\n",
       "       [0.10568666],\n",
       "       [0.12223208],\n",
       "       [0.11939865],\n",
       "       [0.10770267],\n",
       "       [0.17074144],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.12106395],\n",
       "       [0.15128008],\n",
       "       [0.10214955],\n",
       "       [0.11118451],\n",
       "       [0.1640293 ],\n",
       "       [0.11939341],\n",
       "       [0.15262088],\n",
       "       [0.12447727],\n",
       "       [0.12171233],\n",
       "       [0.12197405],\n",
       "       [0.1144926 ],\n",
       "       [0.10137385],\n",
       "       [0.11585891],\n",
       "       [0.10215548],\n",
       "       [0.10721961],\n",
       "       [0.09587324],\n",
       "       [0.16288665],\n",
       "       [0.11571634],\n",
       "       [0.09587324],\n",
       "       [0.12842089],\n",
       "       [0.09587324],\n",
       "       [0.1063368 ],\n",
       "       [0.1629658 ],\n",
       "       [0.09744519],\n",
       "       [0.11894616],\n",
       "       [0.08987272],\n",
       "       [0.11424145],\n",
       "       [0.09587324],\n",
       "       [0.16846776],\n",
       "       [0.11222515],\n",
       "       [0.09587324],\n",
       "       [0.10791361],\n",
       "       [0.10024768],\n",
       "       [0.1045076 ],\n",
       "       [0.17268908],\n",
       "       [0.11563408],\n",
       "       [0.09587324],\n",
       "       [0.15873614],\n",
       "       [0.12055144],\n",
       "       [0.10522491],\n",
       "       [0.10733497],\n",
       "       [0.12432364],\n",
       "       [0.16662544],\n",
       "       [0.09587324],\n",
       "       [0.1372442 ],\n",
       "       [0.11755976],\n",
       "       [0.10896215],\n",
       "       [0.09898877],\n",
       "       [0.10230106],\n",
       "       [0.10802025],\n",
       "       [0.09587324],\n",
       "       [0.0994235 ],\n",
       "       [0.16121793],\n",
       "       [0.09587324],\n",
       "       [0.10826617],\n",
       "       [0.11381754],\n",
       "       [0.1421459 ],\n",
       "       [0.12056404],\n",
       "       [0.16116783],\n",
       "       [0.1698659 ],\n",
       "       [0.13077414],\n",
       "       [0.1405156 ],\n",
       "       [0.14022076],\n",
       "       [0.10610965],\n",
       "       [0.17389107],\n",
       "       [0.11221999],\n",
       "       [0.09587324],\n",
       "       [0.10221061],\n",
       "       [0.10272422],\n",
       "       [0.17004824],\n",
       "       [0.1074509 ],\n",
       "       [0.1027323 ],\n",
       "       [0.09966981],\n",
       "       [0.14879501],\n",
       "       [0.1311715 ],\n",
       "       [0.11327201],\n",
       "       [0.12430486],\n",
       "       [0.1181511 ],\n",
       "       [0.11983287],\n",
       "       [0.11700931],\n",
       "       [0.12093163],\n",
       "       [0.09587324],\n",
       "       [0.10486811],\n",
       "       [0.11956754],\n",
       "       [0.12544647],\n",
       "       [0.11783078],\n",
       "       [0.12900251],\n",
       "       [0.1135546 ],\n",
       "       [0.15904191],\n",
       "       [0.1120548 ],\n",
       "       [0.10598212],\n",
       "       [0.10828266],\n",
       "       [0.14270312],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.12114877],\n",
       "       [0.10840949],\n",
       "       [0.10855147],\n",
       "       [0.09538922],\n",
       "       [0.11931863],\n",
       "       [0.11838517],\n",
       "       [0.1225628 ],\n",
       "       [0.11438507],\n",
       "       [0.10003752],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.12365013],\n",
       "       [0.11603284],\n",
       "       [0.11602306],\n",
       "       [0.17074144],\n",
       "       [0.09587324],\n",
       "       [0.10840949],\n",
       "       [0.11257064],\n",
       "       [0.1053859 ],\n",
       "       [0.08529317],\n",
       "       [0.10999399],\n",
       "       [0.12439075],\n",
       "       [0.09587324],\n",
       "       [0.14180359],\n",
       "       [0.11082861],\n",
       "       [0.09587324],\n",
       "       [0.10272422],\n",
       "       [0.12607807],\n",
       "       [0.09587324],\n",
       "       [0.12607008],\n",
       "       [0.1150375 ],\n",
       "       [0.1056551 ],\n",
       "       [0.12141988],\n",
       "       [0.12551153],\n",
       "       [0.10974532],\n",
       "       [0.13604042],\n",
       "       [0.13243237],\n",
       "       [0.1202561 ],\n",
       "       [0.11981386],\n",
       "       [0.11211014],\n",
       "       [0.11397696],\n",
       "       [0.13276944],\n",
       "       [0.11129388],\n",
       "       [0.11446375],\n",
       "       [0.09912491],\n",
       "       [0.10460779],\n",
       "       [0.09587324],\n",
       "       [0.10973111],\n",
       "       [0.153121  ],\n",
       "       [0.09587324],\n",
       "       [0.10605264],\n",
       "       [0.12072015],\n",
       "       [0.11391854],\n",
       "       [0.11609682],\n",
       "       [0.15540344],\n",
       "       [0.08166596],\n",
       "       [0.12935215],\n",
       "       [0.11114821],\n",
       "       [0.1173653 ],\n",
       "       [0.11526909],\n",
       "       [0.13020256],\n",
       "       [0.11942232],\n",
       "       [0.10024768],\n",
       "       [0.17074144],\n",
       "       [0.09587324],\n",
       "       [0.09883592],\n",
       "       [0.13095737],\n",
       "       [0.11304006],\n",
       "       [0.10871696],\n",
       "       [0.16006592],\n",
       "       [0.09966981],\n",
       "       [0.10476857],\n",
       "       [0.12160847],\n",
       "       [0.10024768],\n",
       "       [0.09966981],\n",
       "       [0.1056551 ],\n",
       "       [0.09587324],\n",
       "       [0.12139013],\n",
       "       [0.09587324],\n",
       "       [0.09674007],\n",
       "       [0.16628668],\n",
       "       [0.1576181 ],\n",
       "       [0.10024768],\n",
       "       [0.09114039],\n",
       "       [0.1102584 ],\n",
       "       [0.12128976],\n",
       "       [0.10003752],\n",
       "       [0.09995633],\n",
       "       [0.11637419],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.10026541],\n",
       "       [0.15705138],\n",
       "       [0.10709661],\n",
       "       [0.12293211],\n",
       "       [0.13104662],\n",
       "       [0.10056487],\n",
       "       [0.1058782 ],\n",
       "       [0.11771363],\n",
       "       [0.11650759],\n",
       "       [0.08684936],\n",
       "       [0.11664891],\n",
       "       [0.12114191],\n",
       "       [0.1167233 ],\n",
       "       [0.09587324],\n",
       "       [0.10271132],\n",
       "       [0.10721347],\n",
       "       [0.1693995 ],\n",
       "       [0.10262451],\n",
       "       [0.11226279],\n",
       "       [0.12775171],\n",
       "       [0.10347372],\n",
       "       [0.14788467],\n",
       "       [0.11011434],\n",
       "       [0.10024768],\n",
       "       [0.12849265],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.12746626],\n",
       "       [0.11639109],\n",
       "       [0.14815876],\n",
       "       [0.12115413],\n",
       "       [0.10384944],\n",
       "       [0.14792255],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.1209121 ],\n",
       "       [0.11118415],\n",
       "       [0.11453065],\n",
       "       [0.09587324],\n",
       "       [0.10527149],\n",
       "       [0.11099279],\n",
       "       [0.17074144],\n",
       "       [0.16801423],\n",
       "       [0.11492422],\n",
       "       [0.17074144],\n",
       "       [0.11611164],\n",
       "       [0.14369926],\n",
       "       [0.09587324],\n",
       "       [0.13001749],\n",
       "       [0.10498774],\n",
       "       [0.10583761],\n",
       "       [0.12274715],\n",
       "       [0.11442566],\n",
       "       [0.12573275],\n",
       "       [0.1567708 ],\n",
       "       [0.1214166 ],\n",
       "       [0.12051708],\n",
       "       [0.09594557],\n",
       "       [0.15767634],\n",
       "       [0.13156047],\n",
       "       [0.11397696],\n",
       "       [0.17109862],\n",
       "       [0.10996529],\n",
       "       [0.09587324],\n",
       "       [0.10855848],\n",
       "       [0.12842089],\n",
       "       [0.11948907],\n",
       "       [0.09587324],\n",
       "       [0.11613318],\n",
       "       [0.15967825],\n",
       "       [0.1131888 ],\n",
       "       [0.09538922],\n",
       "       [0.11781985],\n",
       "       [0.10453498],\n",
       "       [0.09587324],\n",
       "       [0.16650525],\n",
       "       [0.09587324],\n",
       "       [0.1095635 ],\n",
       "       [0.11554396],\n",
       "       [0.10273775],\n",
       "       [0.11945921],\n",
       "       [0.1265524 ],\n",
       "       [0.16663331],\n",
       "       [0.10611328],\n",
       "       [0.11468205],\n",
       "       [0.11638853],\n",
       "       [0.12280914],\n",
       "       [0.11888623],\n",
       "       [0.15010417],\n",
       "       [0.11287874],\n",
       "       [0.12495068],\n",
       "       [0.13883889],\n",
       "       [0.09587324],\n",
       "       [0.13506293],\n",
       "       [0.12970805],\n",
       "       [0.10471272],\n",
       "       [0.11280695],\n",
       "       [0.10520753],\n",
       "       [0.11731741],\n",
       "       [0.12907362],\n",
       "       [0.10210007],\n",
       "       [0.09587324],\n",
       "       [0.10339487],\n",
       "       [0.14331993],\n",
       "       [0.11347756],\n",
       "       [0.10024768],\n",
       "       [0.12106878],\n",
       "       [0.11619297],\n",
       "       [0.10889196],\n",
       "       [0.09587324],\n",
       "       [0.17074144],\n",
       "       [0.11500978],\n",
       "       [0.09587324],\n",
       "       [0.09114039],\n",
       "       [0.12306497],\n",
       "       [0.16986486],\n",
       "       [0.09966981],\n",
       "       [0.09587324],\n",
       "       [0.09505835],\n",
       "       [0.12234181],\n",
       "       [0.09537566],\n",
       "       [0.11100355],\n",
       "       [0.09587324],\n",
       "       [0.1041829 ],\n",
       "       [0.09587324],\n",
       "       [0.12616006],\n",
       "       [0.0998787 ],\n",
       "       [0.12900293],\n",
       "       [0.11837533],\n",
       "       [0.10024768],\n",
       "       [0.12013045],\n",
       "       [0.1225628 ],\n",
       "       [0.10522491],\n",
       "       [0.10130298],\n",
       "       [0.11815998],\n",
       "       [0.09912491],\n",
       "       [0.14499718],\n",
       "       [0.09789461],\n",
       "       [0.12075484],\n",
       "       [0.09587324],\n",
       "       [0.15264535],\n",
       "       [0.10024768],\n",
       "       [0.09587324],\n",
       "       [0.10159895],\n",
       "       [0.13622373],\n",
       "       [0.10852715],\n",
       "       [0.13309395],\n",
       "       [0.17074144],\n",
       "       [0.10321954],\n",
       "       [0.12578896],\n",
       "       [0.11543909],\n",
       "       [0.12235564],\n",
       "       [0.10852715],\n",
       "       [0.12598586],\n",
       "       [0.17074144],\n",
       "       [0.10192505],\n",
       "       [0.13125956],\n",
       "       [0.11904162],\n",
       "       [0.12152305],\n",
       "       [0.10291871],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.09587324],\n",
       "       [0.12157306],\n",
       "       [0.112836  ],\n",
       "       [0.15056592],\n",
       "       [0.10575309],\n",
       "       [0.11626294],\n",
       "       [0.11609131],\n",
       "       [0.11392641],\n",
       "       [0.13733667],\n",
       "       [0.11970314],\n",
       "       [0.09587324],\n",
       "       [0.1300191 ],\n",
       "       [0.11402464],\n",
       "       [0.10326919],\n",
       "       [0.09587324],\n",
       "       [0.10681298],\n",
       "       [0.11937514],\n",
       "       [0.11699194],\n",
       "       [0.09995633],\n",
       "       [0.15680894],\n",
       "       [0.12102598],\n",
       "       [0.13002592],\n",
       "       [0.09587324],\n",
       "       [0.09969762],\n",
       "       [0.10884875],\n",
       "       [0.10615417],\n",
       "       [0.10177273],\n",
       "       [0.16239625],\n",
       "       [0.1259593 ],\n",
       "       [0.1053859 ],\n",
       "       [0.09587324],\n",
       "       [0.16899061],\n",
       "       [0.11127704],\n",
       "       [0.10554984],\n",
       "       [0.11807504],\n",
       "       [0.12369618],\n",
       "       [0.16100383],\n",
       "       [0.15843174],\n",
       "       [0.11308295],\n",
       "       [0.15942082],\n",
       "       [0.16230619],\n",
       "       [0.11416632],\n",
       "       [0.15857661],\n",
       "       [0.12394071],\n",
       "       [0.09587324],\n",
       "       [0.11496222],\n",
       "       [0.12431538],\n",
       "       [0.12489909],\n",
       "       [0.1508055 ],\n",
       "       [0.12944603],\n",
       "       [0.12129006],\n",
       "       [0.09537566],\n",
       "       [0.11804661],\n",
       "       [0.16620123],\n",
       "       [0.1023754 ],\n",
       "       [0.10342903],\n",
       "       [0.09966987],\n",
       "       [0.11879972]], dtype=float32)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-liberty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-monitor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-collect",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-dutch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-fossil",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-campus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-houston",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-woman",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-jamaica",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-holocaust",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-execution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rocky-resident",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Clear target multiple copies effect inefficiency '\n",
      " ' TODO This nd part method ' ' create new empty project' ... ' ok'\n",
      " ' TODO returns Collection single Object Is callers expect '\n",
      " ' see org argouml cognitive critics Critic toDoItem java lang Object org argouml cognitive Designer ']\n"
     ]
    }
   ],
   "source": [
    "print((X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "entire-refund",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7561,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "caroline-nightlife",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[265, 1167]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "charming-presence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7561, 5351)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrices.shape #not be 5351, should be median remove empty, 95\n",
    "#tokenizer hugging face wala use kar, has truncate keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "turkish-schema",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "available-colorado",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(X_train, key = lambda x : len(x))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "german-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                          lower=True)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "reported-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences([\"this perhaps not really necessary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "increasing-irrigation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[14, 918, 320, 152, 514]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "continued-detector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7561,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "floating-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = tokenizer.texts_to_matrix(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "likely-mailing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-instrument",
   "metadata": {},
   "source": [
    "# Test your prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "rotary-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    # Positive Reviews\n",
    "\n",
    "    'This is an excellent movie',\n",
    "    'The move was fantastic I like it',\n",
    "    'You should watch it is brilliant',\n",
    "    'Exceptionally good',\n",
    "    'Wonderfully directed and executed I like it',\n",
    "    'Its a fantastic series',\n",
    "    'Never watched such a brillent movie',\n",
    "    'It is a Wonderful movie',\n",
    "\n",
    "    # Negtive Reviews\n",
    "\n",
    "    \"horrible acting\",\n",
    "    'waste of money',\n",
    "    'pathetic picture',\n",
    "    'It was very boring',\n",
    "    'I did not like the movie',\n",
    "    'The movie was horrible',\n",
    "    'I will not recommend',\n",
    "    'The acting is pathetic'\n",
    "]\n",
    "sentiments = np.array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "physical-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = np.array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "arabic-desire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "#Embedding Layer (wordVocab,wordDim,inputlen) -- transforms words as rows, dimesions as cols matrx\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "all_words = []\n",
    "for sent in corpus:\n",
    "    tokenize_word = word_tokenize(sent)\n",
    "    #tokenize_word is a list of all words in a sentence\n",
    "    for word in tokenize_word:\n",
    "        all_words.append(word)\n",
    "#all_words\n",
    "#we only need unique words \n",
    "unique_words = set(all_words)\n",
    "print(len(unique_words))\n",
    "vocab_length = len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "confirmed-bulgarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33, 25, 1, 12, 13], [35, 19, 19, 4, 30, 14, 36], [37, 37, 31, 36, 25, 40], [27, 7], [25, 10, 10, 5, 30, 14, 36], [40, 30, 4, 36], [32, 12, 26, 30, 10, 13], [36, 25, 30, 3, 13], [44, 28], [6, 5, 1], [10, 6], [36, 19, 24, 6], [30, 31, 19, 14, 35, 13], [35, 13, 19, 44], [30, 13, 19, 30], [35, 28, 25, 10]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "embedded_sentences = [one_hot(sent, vocab_length) for sent in corpus]\n",
    "print(embedded_sentences)\n",
    "#but all these are different lengths so pad them equal to len if longest sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "subtle-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "longest_sentence = max(corpus, key=word_count)\n",
    "length_long_sentence = len(word_tokenize(longest_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "corporate-barrier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33 25  1 12 13  0  0]\n",
      " [35 19 19  4 30 14 36]\n",
      " [37 37 31 36 25 40  0]\n",
      " [27  7  0  0  0  0  0]\n",
      " [25 10 10  5 30 14 36]\n",
      " [40 30  4 36  0  0  0]\n",
      " [32 12 26 30 10 13  0]\n",
      " [36 25 30  3 13  0  0]\n",
      " [44 28  0  0  0  0  0]\n",
      " [ 6  5  1  0  0  0  0]\n",
      " [10  6  0  0  0  0  0]\n",
      " [36 19 24  6  0  0  0]\n",
      " [30 31 19 14 35 13  0]\n",
      " [35 13 19 44  0  0  0]\n",
      " [30 13 19 30  0  0  0]\n",
      " [35 28 25 10  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
    "print(padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ultimate-catering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = Embedding(vocab_length, 20, input_length=length_long_sentence)\n",
    "e.output_dim #16 * 7* 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fluid-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_length, 20, input_length=length_long_sentence))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "virtual-killing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 7, 20)             900       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 140)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 141       \n",
      "=================================================================\n",
      "Total params: 1,041\n",
      "Trainable params: 1,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "purple-swift",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6940 - acc: 0.4375\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6906 - acc: 0.5000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6873 - acc: 0.6250\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6839 - acc: 0.6875\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6805 - acc: 0.7500\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6771 - acc: 0.7500\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6738 - acc: 0.8750\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6704 - acc: 0.8750\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6670 - acc: 0.8750\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6637 - acc: 0.8750\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6603 - acc: 0.8750\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6569 - acc: 0.8750\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6535 - acc: 0.9375\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6501 - acc: 0.9375\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6467 - acc: 0.9375\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6433 - acc: 0.9375\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6398 - acc: 0.9375\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6363 - acc: 0.9375\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6328 - acc: 0.9375\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6293 - acc: 0.9375\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6258 - acc: 0.9375\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6222 - acc: 0.9375\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6187 - acc: 0.9375\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6150 - acc: 0.9375\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6114 - acc: 0.9375\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6077 - acc: 0.9375\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6041 - acc: 0.9375\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6003 - acc: 0.9375\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5966 - acc: 0.9375\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5928 - acc: 0.9375\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5890 - acc: 0.9375\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5851 - acc: 0.9375\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5812 - acc: 0.9375\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5773 - acc: 0.9375\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5734 - acc: 0.9375\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5694 - acc: 0.9375\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5654 - acc: 0.9375\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5614 - acc: 0.9375\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5573 - acc: 0.9375\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5532 - acc: 0.9375\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5491 - acc: 0.9375\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5449 - acc: 0.9375\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5407 - acc: 0.9375\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5365 - acc: 0.9375\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5322 - acc: 0.9375\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5279 - acc: 0.9375\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5236 - acc: 0.9375\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5193 - acc: 0.9375\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5149 - acc: 0.9375\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5106 - acc: 0.9375\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5062 - acc: 0.9375\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.5017 - acc: 0.9375\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4973 - acc: 0.9375\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4928 - acc: 0.9375\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4883 - acc: 0.9375\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4838 - acc: 0.9375\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4793 - acc: 0.9375\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4748 - acc: 0.9375\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4702 - acc: 0.9375\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4657 - acc: 0.9375\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4611 - acc: 0.9375\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4565 - acc: 0.9375\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4519 - acc: 0.9375\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4474 - acc: 0.9375\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4428 - acc: 0.9375\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4382 - acc: 0.9375\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4336 - acc: 0.9375\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4290 - acc: 0.9375\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4244 - acc: 0.9375\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4198 - acc: 0.9375\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4152 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4106 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4060 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.4015 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3969 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3924 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3878 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3833 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3788 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3743 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3699 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3654 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3610 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3566 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3522 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3478 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3435 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3392 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3349 - acc: 1.0000\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3306 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3264 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3222 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3180 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3139 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3098 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3057 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.3017 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2977 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2937 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2897 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18307e5dee0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_sentences, sentiments, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "independent-fence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-collar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
