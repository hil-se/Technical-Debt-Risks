{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x000001AFE90B49E8>\n",
      "Abstract          11220\n",
      "clean_comment      9325\n",
      "projectname          31\n",
      "classification       22\n",
      "ID                    5\n",
      "label                 3\n",
      "dtype: int64\n",
      "\n",
      "\n",
      " [('the', 1), ('to', 2), ('a', 3), ('\\r', 4), ('is', 5), ('of', 6), ('this', 7), ('nonnls1', 8), ('for', 9), ('we', 10), ('in', 11), ('see', 12), ('if', 13), ('and', 14), ('not', 15), ('method', 16), ('be', 17), ('that', 18), ('it', 19), ('todo', 20), ('are', 21), ('as', 22), ('with', 23), ('value', 24), ('from', 25)]\n",
      "\n",
      "\n",
      "Shape of X train and X validation tensor: (45902, 2000) (11476, 2000)\n",
      "\n",
      "\n",
      "Shape of label train and validation tensor: (45902, 2) (11476, 2)\n",
      "\n",
      "Found 27653 unique tokens in embeddings func.\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.08007812  0.10498047  0.04980469 ...  0.00366211  0.04760742\n",
      "  -0.06884766]\n",
      " [ 0.04613327 -0.08605109 -0.24534731 ...  0.63720402 -0.60182996\n",
      "   0.4588261 ]\n",
      " ...\n",
      " [ 0.01611328  0.14550781  0.22265625 ... -0.08984375  0.10986328\n",
      "  -0.0534668 ]\n",
      " [ 0.09716797  0.11035156  0.02954102 ... -0.01080322 -0.14453125\n",
      "   0.15820312]\n",
      " [-0.23144531  0.05151367 -0.13867188 ... -0.08984375 -0.03466797\n",
      "  -0.07617188]]\n",
      "2000\n",
      "\n",
      "\n",
      "Shape of embedding matrix:  (2000, 300)\n",
      "number of null word embeddings: 1\n",
      "Tensor(\"embedding_1/embedding_lookup/Identity_1:0\", shape=(None, 2000, 300), dtype=float32) \n",
      "\n",
      "\n",
      "\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 2000, 300)    600000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1999, 64)     38464       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 1998, 64)     57664       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 1997, 64)     76864       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 1996, 64)     96064       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1995, 64)     115264      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 64)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 64)           0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 64)           0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 64)           0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 320)          0           global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 320)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           20544       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            130         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,004,994\n",
      "Trainable params: 404,994\n",
      "Non-trainable params: 600,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class tech_debt_simple_cnn:\n",
    "    \n",
    "    def get_data(self):\n",
    "        d1=pd.read_csv(\"D:/RIT/GA-TECHNICAL DEBTS/rudimentary-stages/data/all/sql12.csv\")\n",
    "        d2=pd.read_csv(\"D:/RIT/GA-TECHNICAL DEBTS/rudimentary-stages/data/all/apache-ant-1.7.0.csv\")\n",
    "        d3=pd.read_csv(\"D:/RIT/GA-TECHNICAL DEBTS/rudimentary-stages/data/all/apache-jmeter-2.10.csv\")\n",
    "        d4=pd.read_csv(\"D:/RIT/GA-TECHNICAL DEBTS/rudimentary-stages/data/all/argouml.csv\")\n",
    "        d5=pd.read_csv(\"D:/RIT/GA-TECHNICAL DEBTS/rudimentary-stages/data/all/columba-1.4-src.csv\")\n",
    "        d6=pd.read_csv(\"D:/RIT/GA-TECHNICAL DEBTS/rudimentary-stages/data/all/emf-2.4.1.csv\")\n",
    "        d7=pd.read_csv(\"D:/RIT/GA-TECHNICAL DEBTS/rudimentary-stages/data/all/hibernate-distribution-3.3.2.GA.csv\")\n",
    "        d8=pd.read_csv(\"D:/RIT/GA-TECHNICAL DEBTS/rudimentary-stages/data/all/jEdit-4.2.csv\")\n",
    "        d9=pd.read_csv(\"D:/RIT/GA-TECHNICAL DEBTS/rudimentary-stages/data/all/jfreechart-1.0.19.csv\")\n",
    "        frames=[d1,d2,d3,d4,d5,d6,d7,d8,d9]\n",
    "        data=pd.concat(frames)\n",
    "        #print(data.head())\n",
    "        \n",
    "        \n",
    "        '''Remove punctuations'''       \n",
    "        import re\n",
    "        import string\n",
    "        def remove_punct(text):\n",
    "            text_nopunct = ''\n",
    "            text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "            return text_nopunct\n",
    "        data['clean_comment'] = data['Abstract'].apply(lambda x: remove_punct(x))\n",
    "        \n",
    "        '''Finding max length of comment in our dataset -11220 after punct - 9325'''\n",
    "        mx_dct = {c: data[c].map(lambda x: len(str(x))).max() for c in data.columns}\n",
    "        print(pd.Series(mx_dct).sort_values(ascending =False)) #print maxlen\n",
    "                \n",
    "        #print(data.head()) #see clean comments column\n",
    "        \n",
    "        comments=data[['clean_comment']]\n",
    "        labels=data[['label']]\n",
    "        labels_list=[]\n",
    "        for i, row in labels.iterrows():\n",
    "            labels_list.append(row['label'])\n",
    "\n",
    "        comments_list=[]\n",
    "        for i, row in comments.iterrows():\n",
    "            comments_list.append(row['clean_comment'])\n",
    "            \n",
    "        #print(\"\\n\\n\", comments_list[:10], \"\\n\\n\\n\")\n",
    "        #print(\"\\n\\n\",labels_list[:10])\n",
    "        \n",
    "        return comments_list,labels_list,data\n",
    "    \n",
    "    def preprocess(self):\n",
    "        \n",
    "        from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "        from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "        from tensorflow.keras.utils import to_categorical\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "        c, l,data= self.get_data()\n",
    "        #print(\"\\n\\n\", c[:20], \"\\n\\n\\n\")\n",
    "        #print(\"\\n\\n\",l[:20])\n",
    "        \n",
    "        NUM_WORDS=2000 #must be 9325 actually, also do we include \\r in filters? \n",
    "        tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                              lower=True)\n",
    "        tokenizer.fit_on_texts(c)\n",
    "        X = tokenizer.texts_to_sequences(c)\n",
    "        X,l= np.array(X), np.array(l)\n",
    "        word_index = tokenizer.word_index\n",
    "        #print('\\nFound %s unique tokens.' % len(word_index))       \n",
    "\n",
    "        '''print word  index example'''\n",
    "        print(\"\\n\\n\", list(word_index.items())[:25])\n",
    "      \n",
    "        '''verify length of   word index = unique words found '''\n",
    "        #print(len(word_index))\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        vec = label_encoder.fit_transform(l)\n",
    "        y=to_categorical(vec)\n",
    "        \n",
    "        X = pad_sequences(X,maxlen=NUM_WORDS)\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "        data = {}\n",
    "        data[\"X_train\"] = X_train\n",
    "        data[\"X_val\"]= X_val\n",
    "        data[\"y_train\"] = y_train\n",
    "        data[\"y_val\"] = y_val\n",
    "        data[\"tokenizer\"] = tokenizer\n",
    "        data[\"int2label\"] =  {0: \"no\", 1: \"yes\"}\n",
    "        data[\"label2int\"] = {\"no\": 0, \"yes\": 1}\n",
    "        \n",
    "        #print(data)\n",
    "        print('\\n\\nShape of X train and X validation tensor:', X_train.shape,X_val.shape)\n",
    "        print('\\n\\nShape of label train and validation tensor:', y_train.shape,y_val.shape)\n",
    "        return word_index,X_train, X_val, y_train, y_val\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        \n",
    "        import gensim\n",
    "        from gensim.models import Word2Vec\n",
    "        from gensim.utils import simple_preprocess\n",
    "\n",
    "        from gensim.models.keyedvectors import KeyedVectors\n",
    "        word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        \n",
    "        #load pre trained weights\n",
    "        print(word_vectors)\n",
    "        \n",
    "        word_index,X_train, X_val, y_train, y_val=self.preprocess()\n",
    "        print('\\nFound %s unique tokens in embeddings func.' % len(word_index))       \n",
    "        \n",
    "        EMBEDDING_DIM=300\n",
    "        NUM_WORDS=2000\n",
    "        vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "        embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "        for word, i in word_index.items():\n",
    "            if i>=NUM_WORDS:\n",
    "                continue\n",
    "            try:\n",
    "                embedding_vector = word_vectors[word]\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            except KeyError:\n",
    "                embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "        print(embedding_matrix)\n",
    "        \n",
    "        sequence_length = X_train.shape[1]\n",
    "        print(sequence_length)\n",
    "        \n",
    "        return embedding_matrix, word_index, X_train, X_val, y_train, y_val\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "        from tensorflow.keras.models import Model, Sequential\n",
    "        from tensorflow.keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate,GlobalMaxPooling1D\n",
    "        from tensorflow.keras.layers import Reshape, Flatten\n",
    "        from tensorflow.keras.callbacks import EarlyStopping\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        from tensorflow.keras.models import Model\n",
    "        from tensorflow.keras import regularizers\n",
    "        from tensorflow.keras import optimizers\n",
    "        \n",
    "        embedding_matrix,word_index, X_train, X_val, y_train, y_val =self.get_embeddings()\n",
    "        print(\"\\n\\nShape of embedding matrix: \",embedding_matrix.shape)\n",
    "        print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "        \n",
    "        EMBEDDING_DIM=300\n",
    "        NUM_WORDS=2000\n",
    "        vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "        embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)\n",
    "        \n",
    "        sequence_length =2000\n",
    "        #have to put these in init\n",
    "        batch_size = 32 \n",
    "        num_epochs = 10\n",
    "\n",
    "        #model parameters\n",
    "        num_filters = 256 #put this later \n",
    "        weight_decay = 1e-4\n",
    "        sequence_input = Input(shape=(sequence_length,))\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "        print(embedded_sequences, \"\\n\\n\\n\")\n",
    "        \n",
    "        convs = []\n",
    "        filter_sizes = [2,3,4,5,6]\n",
    "        for filter_size in filter_sizes:\n",
    "            l_conv = Conv1D(filters=64, \n",
    "                             kernel_size=filter_size, \n",
    "                            activation='relu')(embedded_sequences)\n",
    "            l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "            convs.append(l_pool)\n",
    "        l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "        x = Dropout(0.1)(l_merge)  \n",
    "        x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "        preds = Dense(2, activation='softmax')(x)\n",
    "        adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)      \n",
    "        model = Model(sequence_input, preds)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "        callbacks_list = [early_stopping]\n",
    "        \n",
    "        #train model\n",
    "        def train(self):\n",
    "            hist = model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, \n",
    "                validation_split=0.2, shuffle=True, verbose=2)\n",
    "       \n",
    "            model_name=\"v1-cnn-word2vec\"      \n",
    "\n",
    "            scores = model.evaluate(X_val, y_val, verbose=0)\n",
    "            print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "            #save model \n",
    "            if not os.path.isdir(\"results\"):\n",
    "                os.mkdir(\"results\")\n",
    "            if not os.path.isdir(\"logs\"):\n",
    "                os.mkdir(\"logs\")\n",
    "            if not os.path.isdir(\"data\"):\n",
    "                os.mkdir(\"data\")\n",
    "            model_name=\"cnn-imdb\"\n",
    "            model.save(os.path.join(\"results\", model_name) + \".h5\")\n",
    "            return\n",
    "\n",
    "        def show(self):\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure()\n",
    "            plt.plot(ist.history['accuracy'], lw=2.0, color='b', label='train')\n",
    "            plt.plot(ist.history['val_accuracy'], lw=2.0, color='r', label='val')\n",
    "            plt.title('CNN label')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend(loc='upper left')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(ist.history['loss'], lw=2.0, color='b', label='train')\n",
    "            plt.plot(ist.history['val_loss'], lw=2.0, color='r', label='val')\n",
    "            plt.title('CNN label')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Cross-Entropy Loss')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.show()\n",
    "        \n",
    "        #self.train()\n",
    "        #self.show()\n",
    "        \n",
    "        def test(self, model):\n",
    "            import tensorflow as tf\n",
    "            dtest=pd.read_csv(\"D:/RIT/GA-TECHNICAL DEBTS/rudimentary-stages/data/all/jruby-1.4.0.csv\")\n",
    "            c=dtest[['Abstract']]\n",
    "            l=dtest[['label']]\n",
    "            labels_test=[]\n",
    "            for i, row in l.iterrows():\n",
    "                labels_test.append(row['label'])\n",
    "\n",
    "            comments_test=[]\n",
    "            for i, row in c.iterrows():\n",
    "                comments_test.append(row['Abstract'])\n",
    "                \n",
    "                \n",
    "            NUM_WORDS=2000\n",
    "            tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                                  lower=True)\n",
    "            tokenizer.fit_on_texts(comments_test)\n",
    "            Xtest = tokenizer.texts_to_sequences(comments_test)\n",
    "            Xtest,ll= np.array(Xtest), np.array(labels_test)\n",
    "            Xtest = pad_sequences(Xtest,maxlen=2000)\n",
    "            \n",
    "            label_encoder = LabelEncoder()\n",
    "            vec1 = label_encoder.fit_transform(ll)\n",
    "            ytest=to_categorical(vec1)\n",
    "            \n",
    "            scores1 = model.evaluate(Xtest, ytest, verbose=0)\n",
    "            print(\"Test Accuracy: %.2f%%\" % (scores1[1]*100))\n",
    "            \n",
    "            y_pred=model.predict(Xtest)\n",
    "            \n",
    "            cm=tf.math.confusion_matrix(labels=tf.argmax(ytest, 1), predictions=tf.argmax(y_pred, 1))\n",
    "            \n",
    "            print(cm)\n",
    "            return\n",
    "        \n",
    "        #self.show()\n",
    "        \n",
    "    \n",
    "        \n",
    "ob=tech_debt_simple_cnn()\n",
    "ob.build_model()\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
